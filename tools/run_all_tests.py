#!/usr/bin/env python3
import json
import sys
import os
import subprocess
import argparse
from pathlib import Path

# –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ tools, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å utils
sys.path.insert(0, str(Path(__file__).parent))
from utils import make_task_result_stub

def run_behavioral_test(task_config):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (–∫–∞–∫ —Ä–∞–Ω—å—à–µ)"""
    task_id = task_config["id"]
    task_file = task_config["file"]
    results_dir = Path("results")
    results_dir.mkdir(exist_ok=True)
    result_path = results_dir / f"{task_id}.json"

    if not os.path.exists(task_file):
        print(f"‚ö†Ô∏è {task_file} –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî —Å–æ–∑–¥–∞—ë–º –∑–∞–≥–ª—É—à–∫—É")
        stub = make_task_result_stub(task_config)
        with open(result_path, "w", encoding="utf-8") as f:
            json.dump(stub, f, ensure_ascii=False, indent=2)
        return

    tests = []
    total_score = 0

    for test in task_config["tests"]:
        try:
            result = subprocess.run(
                ["python3", task_file],
                input=test["input"],
                text=True,
                capture_output=True,
                timeout=5
            )
            output = result.stdout.strip()
            expected = test["expected_output"]
            method = test["comparison_method"]

            if method == "exact":
                passed = output == expected
            elif method == "contains":
                passed = expected in output
            else:
                passed = False

            score = test["max_score"] if passed else 0
            total_score += score

            tests.append({
                "name": test["name"],
                "status": "pass" if passed else "fail",
                "score": score,
                "output": output[:200]  # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π –≤—ã–≤–æ–¥
            })

        except subprocess.TimeoutExpired:
            tests.append({
                "name": test["name"],
                "status": "fail",
                "score": 0,
                "output": "TIMEOUT"
            })
        except Exception as e:
            tests.append({
                "name": test["name"],
                "status": "fail",
                "score": 0,
                "output": f"ERROR: {str(e)}"
            })

    result_data = {
        "version": 1,
        "status": "pass" if total_score == task_config["max_score"] else "fail",
        "max_score": task_config["max_score"],
        "tests": tests
    }

    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)

def run_refactor_check(task_id: str, check_script: str, max_score: int):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Å–∫—Ä–∏–ø—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç results/{task_id}.json"""
    results_dir = Path("results")
    results_dir.mkdir(exist_ok=True)
    result_path = results_dir / f"{task_id}.json"

    try:
        result = subprocess.run(
            ["python3", check_script],
            capture_output=True,
            text=True,
            timeout=10
        )
        passed = result.returncode == 0
        score = max_score if passed else 0
        output = result.stdout.strip() or result.stderr.strip()
    except Exception as e:
        passed = False
        score = 0
        output = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞: {e}"

    test_entry = {
        "name": "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞",
        "status": "pass" if passed else "fail",
        "score": score,
        "output": output[:200]
    }

    result_data = {
        "version": 1,
        "status": "pass" if score == max_score else "fail",
        "max_score": max_score,
        "tests": [test_entry]
    }

    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)

    status = "‚úÖ" if passed else "‚ùå"
    print(f"{status} –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ {task_id}: {output}")

def main():
    config_path = ".github/tasks.json"
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—ã—á–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
    for task in config["tasks"]:
        if not task["id"].endswith("_refactor"):
            print(f"üîç –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –¥–ª—è {task['id']}")
            run_behavioral_test(task)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    refactor_tasks = [
        ("task_01_refactor", "tools/test_refactor_task_01.py", 20),
        ("task_02_refactor", "tools/test_refactor_task_02.py", 20),
    ]

    for task_id, script, max_score in refactor_tasks:
        print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞: {task_id}")
        if os.path.exists(script):
            run_refactor_check(task_id, script, max_score)
        else:
            # –ï—Å–ª–∏ —Å–∫—Ä–∏–ø—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî —Å–æ–∑–¥–∞—ë–º –∑–∞–≥–ª—É—à–∫—É
            results_dir = Path("results")
            results_dir.mkdir(exist_ok=True)
            stub = make_task_result_stub({
                "id": task_id,
                "max_score": max_score,
                "tests": [{"name": "–ü—Ä–æ–≤–µ—Ä–∫–∞", "max_score": max_score}]
            })
            with open(results_dir / f"{task_id}.json", "w") as f:
                json.dump(stub, f, ensure_ascii=False, indent=2)
            print(f"‚ö†Ô∏è –°–∫—Ä–∏–ø—Ç {script} –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî —Å–æ–∑–¥–∞–Ω –∑–∞–≥–ª—É—à–∫–∞-—Ä–µ–∑—É–ª—å—Ç–∞—Ç")

if __name__ == "__main__":
    main()